{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:14.550899Z",
     "start_time": "2024-05-30T07:04:15.597022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = 'https://mediadive.dsmz.de/gas'\n",
    "\n",
    "# Prepare lists to hold the data\n",
    "N2 = []\n",
    "O2 = []\n",
    "CO2 = []\n",
    "H2 = []\n",
    "CH4 = []\n",
    "CO = []\n",
    "Air = []\n",
    "mediums = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the table rows\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Check if rows are empty (end of pagination)\n",
    "    if not rows:\n",
    "        return False\n",
    "    \n",
    "    table_rows = soup.find('div', class_=\"table-responsive my-15\").find_all('tr')[1:]  # Skip the header row\n",
    "    # print(table_rows)\n",
    "    # Loop through the rows and extract data\n",
    "    for row in table_rows:\n",
    "        columns = row.find_all('td')\n",
    "        Medium = columns[0].get_text(strip=True)\n",
    "        mediums.append(Medium)\n",
    "        N_2 = columns[1].get_text(strip=True)\n",
    "        N2.append(N_2)\n",
    "        O_2 = columns[2].get_text(strip=True)\n",
    "        O2.append(O_2)\n",
    "        CO_2 = columns[3].get_text(strip=True)\n",
    "        CO2.append(CO_2)\n",
    "        # print(Medium)\n",
    "        H_2 = columns[4].get_text(strip=True)\n",
    "        H2.append(H_2)\n",
    "        CH_4 = columns[5].get_text(strip=True)\n",
    "        CH4.append(CH_4)\n",
    "        CO1 = columns[6].get_text(strip=True)\n",
    "        CO.append(CO1)\n",
    "        air = columns[7].get_text(strip=True)\n",
    "        Air.append(air)\n",
    "    return True\n",
    "\n",
    "# Loop through the first 100 pages\n",
    "for page in range(1, 49):\n",
    "    page_url = f'{base_url}?p={page}'\n",
    "    print(f'Scraping page: {page}')\n",
    "    if not scrape_page(page_url):\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Medium': mediums,\n",
    "    'N2':N2,\n",
    "    'O2':O2,\n",
    "    'CO2':CO2,\n",
    "    'H2':H2,\n",
    "    'CH4':CH4,\n",
    "    'CO':CO,\n",
    "    'Air':Air\n",
    "})\n",
    "\n",
    "df\n",
    "df.to_csv('data.csv')"
   ],
   "id": "73d2d473739d9e95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: 1\n",
      "Scraping page: 2\n",
      "Scraping page: 3\n",
      "Scraping page: 4\n",
      "Scraping page: 5\n",
      "Scraping page: 6\n",
      "Scraping page: 7\n",
      "Scraping page: 8\n",
      "Scraping page: 9\n",
      "Scraping page: 10\n",
      "Scraping page: 11\n",
      "Scraping page: 12\n",
      "Scraping page: 13\n",
      "Scraping page: 14\n",
      "Scraping page: 15\n",
      "Scraping page: 16\n",
      "Scraping page: 17\n",
      "Scraping page: 18\n",
      "Scraping page: 19\n",
      "Scraping page: 20\n",
      "Scraping page: 21\n",
      "Scraping page: 22\n",
      "Scraping page: 23\n",
      "Scraping page: 24\n",
      "Scraping page: 25\n",
      "Scraping page: 26\n",
      "Scraping page: 27\n",
      "Scraping page: 28\n",
      "Scraping page: 29\n",
      "Scraping page: 30\n",
      "Scraping page: 31\n",
      "Scraping page: 32\n",
      "Scraping page: 33\n",
      "Scraping page: 34\n",
      "Scraping page: 35\n",
      "Scraping page: 36\n",
      "Scraping page: 37\n",
      "Scraping page: 38\n",
      "Scraping page: 39\n",
      "Scraping page: 40\n",
      "Scraping page: 41\n",
      "Scraping page: 42\n",
      "Scraping page: 43\n",
      "Scraping page: 44\n",
      "Scraping page: 45\n",
      "Scraping page: 46\n",
      "Scraping page: 47\n",
      "Scraping page: 48\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T07:37:45.853484Z",
     "start_time": "2024-05-30T07:37:07.197722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = 'https://mediadive.dsmz.de/steps'\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page):\n",
    "    page_url = f'{base_url}?p={page}'\n",
    "    response = requests.get(page_url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the table rows\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Check if rows are empty (end of pagination)\n",
    "    if not rows:\n",
    "        return None, page\n",
    "    \n",
    "    table_rows = soup.find('div', class_=\"table-responsive my-15\").find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    # Temporary lists to hold data for this page\n",
    "    temp_ids = []\n",
    "    temp_steps = []\n",
    "    temp_solutions = []\n",
    "    temp_solutions_link = []\n",
    "    temp_mediums = []\n",
    "    temp_mediums_link = []\n",
    "\n",
    "    # Loop through the rows and extract data\n",
    "    for row in table_rows:\n",
    "        id = row.get('id')\n",
    "        temp_ids.append(id)\n",
    "        \n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        Step = columns[0].get_text(strip=True)\n",
    "        temp_steps.append(Step)\n",
    "        \n",
    "        Solution = columns[1].get_text(strip=True)\n",
    "        temp_solutions.append(Solution)\n",
    "        Solution_link = columns[1].find('a')['href'] if columns[1].find('a') else None\n",
    "        temp_solutions_link.append(Solution_link)\n",
    "        \n",
    "        Medium = columns[2].get_text(strip=True)\n",
    "        temp_mediums.append(Medium)\n",
    "        Medium_link = columns[2].find('a')['href'] if columns[2].find('a') else None\n",
    "        temp_mediums_link.append(Medium_link)\n",
    "    \n",
    "    return (temp_ids, temp_steps, temp_solutions, temp_solutions_link, temp_mediums, temp_mediums_link), page\n",
    "\n",
    "# Number of threads\n",
    "num_threads = 10\n",
    "\n",
    "# Dictionary to hold the results from each page\n",
    "results_dict = {}\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize the scraping process\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = {executor.submit(scrape_page, page): page for page in range(1, 308)}\n",
    "    with tqdm(total=307, desc=\"Scraping pages\", position=0, leave=True) as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result, page = future.result()\n",
    "            if result:\n",
    "                results_dict[page] = result\n",
    "            pbar.update(1)\n",
    "\n",
    "# Lists to hold the final results in order\n",
    "ids = []\n",
    "Steps = []\n",
    "Solutions = []\n",
    "Solutions_link = []\n",
    "Mediums = []\n",
    "Mediums_link = []\n",
    "\n",
    "# Concatenate results in the order of pages\n",
    "with tqdm(total=307, desc=\"Concatenating results\", position=1, leave=True) as pbar:\n",
    "    for page in range(1, 308):\n",
    "        if page in results_dict:\n",
    "            temp_ids, temp_steps, temp_solutions, temp_solutions_link, temp_mediums, temp_mediums_link = results_dict[page]\n",
    "            ids.extend(temp_ids)\n",
    "            Steps.extend(temp_steps)\n",
    "            Solutions.extend(temp_solutions)\n",
    "            Solutions_link.extend(temp_solutions_link)\n",
    "            Mediums.extend(temp_mediums)\n",
    "            Mediums_link.extend(temp_mediums_link)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Step': Steps,\n",
    "    'Solution': Solutions,\n",
    "    'Solution_link': Solutions_link,\n",
    "    'Medium': Mediums,\n",
    "    'Mediums_link': Mediums_link\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('Steps_new.csv', index=False)\n",
    "\n",
    "print('Scraping complete. Data saved to Steps_new.csv')"
   ],
   "id": "ac9baaaa6b3015ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 307/307 [00:38<00:00,  7.95it/s]\n",
      "\n",
      "Concatenating results: 100%|██████████| 307/307 [00:00<00:00, 224095.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to Steps_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:14:13.962496Z",
     "start_time": "2024-06-01T18:13:45.291453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Base URL of the webpage\n",
    "base_url = \"https://mediadive.dsmz.de\"\n",
    "\n",
    "# Function to extract data from a single page\n",
    "def extract_data_from_page(url, page, retries=5):\n",
    "    params = {\"p\": page}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            data = []\n",
    "            table_rows = soup.find_all('tr')[1:]  # Skip the header row\n",
    "            for row in table_rows:\n",
    "                columns = row.find_all('td')\n",
    "                row_data = []\n",
    "                \n",
    "                # ID\n",
    "                ID_group = columns[1].get_text(strip=True)\n",
    "                row_data.append(ID_group)\n",
    "                \n",
    "                # Source\n",
    "                Source_group = columns[2].get_text(strip=True)\n",
    "                row_data.append(Source_group)\n",
    "                \n",
    "                # Name and link\n",
    "                name_tag = columns[3].find('a')\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                name_link = base_url + name_tag['href']\n",
    "                row_data.append(name)\n",
    "                row_data.append(name_link)\n",
    "                \n",
    "                \n",
    "                #Type\n",
    "                types = [a.get_text(strip=True) for a in columns[4].find_all('span')]\n",
    "                row_data.append(types)\n",
    "                \n",
    "                # Final pH\n",
    "                PH_group = columns[5].get_text(strip=True)\n",
    "                row_data.append(PH_group)\n",
    "                \n",
    "                # Tax range\n",
    "                tax_ranges = [a['title'] for a in columns[6].find_all('span', class_=None)] \n",
    "                row_data.append(tax_ranges)\n",
    "                \n",
    "                # Strains\n",
    "                Strains_num = [a.get_text(strip=True) for a in columns[7].find_all('a')]\n",
    "                Strains_link = [base_url + a['href'] for a in columns[7].find_all('a')]\n",
    "                row_data.append(Strains_num)\n",
    "                row_data.append(Strains_link)\n",
    "                \n",
    "                # PDF\n",
    "                pdf = [base_url + a['href'] for a in columns[8].find_all('a')]\n",
    "                row_data.append(pdf)\n",
    "                \n",
    "                data.append(row_data)\n",
    "            \n",
    "            return data\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Error fetching page {page}: {e}. Retrying {attempt + 1}/{retries}...\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "    return []\n",
    "\n",
    "# Main scraping process\n",
    "all_data = []\n",
    "num_pages = 166  # Adjust the number of pages you want to scrape\n",
    "\n",
    "max_workers = 32  # Adjust based on the MacBook M3 Pro capabilities\n",
    "\n",
    "page_data_segments = [None] * num_pages  # Initialize a list to hold data for each page\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_page = {executor.submit(extract_data_from_page, base_url + \"/media\", page): page for page in range(1, num_pages + 1)}\n",
    "    \n",
    "    for future in tqdm(as_completed(future_to_page), total=num_pages, desc=\"Extracting data from pages\"):\n",
    "        page = future_to_page[future]\n",
    "        page_data = future.result()\n",
    "        if page_data:\n",
    "            page_data_segments[page-1] = page_data  # Store the data segment for the corresponding page\n",
    "\n",
    "# Concatenate all page data segments in order\n",
    "for segment in page_data_segments:\n",
    "    if segment:\n",
    "        all_data.extend(segment)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "columns = [\"ID\", \"Source\", \"Name\", \"Name Link\", \"Type\", \"Final PH\", \"Tax.range type\", \"Strains number\", \"Strains links\", \"PDF\"]\n",
    "df = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv('runkun_media', index=False)"
   ],
   "id": "21c8c26fc81fe8b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data from pages:  99%|█████████▉| 165/166 [00:19<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching page 166: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')). Retrying 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data from pages: 100%|██████████| 166/166 [00:28<00:00,  5.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      ID  Source                                        Name  \\\n",
       "0      1    DSMZ                               NUTRIENT AGAR   \n",
       "1     1a    DSMZ           REACTIVATION WITH LIQUID MEDIUM 1   \n",
       "2      2    DSMZ                   BACILLUS PASTEURII MEDIUM   \n",
       "3      3    DSMZ                          AZOTOBACTER MEDIUM   \n",
       "4      6    DSMZ                    ALLANTOIN MINERAL MEDIUM   \n",
       "...   ..     ...                                         ...   \n",
       "3308  P5  public  RS Medium - Nutrient Medium (NM) Component   \n",
       "3309  P6  public                                       mTA10   \n",
       "3310  P7  public     N27 RHODOSPIRILLACEAE MEDIUM (modified)   \n",
       "3311  P8  public                               M9-SNG medium   \n",
       "3312  P9  public                              PDB-SNG medium   \n",
       "\n",
       "                                Name Link       Type Final PH  \\\n",
       "0      https://mediadive.dsmz.de/medium/1  [complex]      7.0   \n",
       "1     https://mediadive.dsmz.de/medium/1a  [complex]      7.0   \n",
       "2      https://mediadive.dsmz.de/medium/2  [complex]            \n",
       "3      https://mediadive.dsmz.de/medium/3  [defined]      7.3   \n",
       "4      https://mediadive.dsmz.de/medium/6  [complex]            \n",
       "...                                   ...        ...      ...   \n",
       "3308  https://mediadive.dsmz.de/medium/P5  [complex]      7.0   \n",
       "3309  https://mediadive.dsmz.de/medium/P6  [complex]      7.2   \n",
       "3310  https://mediadive.dsmz.de/medium/P7  [complex]      6.8   \n",
       "3311  https://mediadive.dsmz.de/medium/P8  [complex]      7.2   \n",
       "3312  https://mediadive.dsmz.de/medium/P9  [complex]      7.2   \n",
       "\n",
       "          Tax.range type Strains number  \\\n",
       "0     [Bacteria, Phages]         [2299]   \n",
       "1             [Bacteria]          [209]   \n",
       "2             [Bacteria]            [9]   \n",
       "3             [Bacteria]           [40]   \n",
       "4             [Bacteria]           [22]   \n",
       "...                  ...            ...   \n",
       "3308                  []             []   \n",
       "3309                  []             []   \n",
       "3310                  []             []   \n",
       "3311                  []             []   \n",
       "3312                  []             []   \n",
       "\n",
       "                                      Strains links  \\\n",
       "0      [https://mediadive.dsmz.de/strains/medium/1]   \n",
       "1     [https://mediadive.dsmz.de/strains/medium/1a]   \n",
       "2      [https://mediadive.dsmz.de/strains/medium/2]   \n",
       "3      [https://mediadive.dsmz.de/strains/medium/3]   \n",
       "4      [https://mediadive.dsmz.de/strains/medium/6]   \n",
       "...                                             ...   \n",
       "3308                                             []   \n",
       "3309                                             []   \n",
       "3310                                             []   \n",
       "3311                                             []   \n",
       "3312                                             []   \n",
       "\n",
       "                                     PDF  \n",
       "0      [https://mediadive.dsmz.de/pdf/1]  \n",
       "1     [https://mediadive.dsmz.de/pdf/1a]  \n",
       "2      [https://mediadive.dsmz.de/pdf/2]  \n",
       "3      [https://mediadive.dsmz.de/pdf/3]  \n",
       "4      [https://mediadive.dsmz.de/pdf/6]  \n",
       "...                                  ...  \n",
       "3308  [https://mediadive.dsmz.de/pdf/P5]  \n",
       "3309  [https://mediadive.dsmz.de/pdf/P6]  \n",
       "3310  [https://mediadive.dsmz.de/pdf/P7]  \n",
       "3311  [https://mediadive.dsmz.de/pdf/P8]  \n",
       "3312  [https://mediadive.dsmz.de/pdf/P9]  \n",
       "\n",
       "[3313 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Name Link</th>\n",
       "      <th>Type</th>\n",
       "      <th>Final PH</th>\n",
       "      <th>Tax.range type</th>\n",
       "      <th>Strains number</th>\n",
       "      <th>Strains links</th>\n",
       "      <th>PDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DSMZ</td>\n",
       "      <td>NUTRIENT AGAR</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/1</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[Bacteria, Phages]</td>\n",
       "      <td>[2299]</td>\n",
       "      <td>[https://mediadive.dsmz.de/strains/medium/1]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1a</td>\n",
       "      <td>DSMZ</td>\n",
       "      <td>REACTIVATION WITH LIQUID MEDIUM 1</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/1a</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[Bacteria]</td>\n",
       "      <td>[209]</td>\n",
       "      <td>[https://mediadive.dsmz.de/strains/medium/1a]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/1a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DSMZ</td>\n",
       "      <td>BACILLUS PASTEURII MEDIUM</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/2</td>\n",
       "      <td>[complex]</td>\n",
       "      <td></td>\n",
       "      <td>[Bacteria]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[https://mediadive.dsmz.de/strains/medium/2]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DSMZ</td>\n",
       "      <td>AZOTOBACTER MEDIUM</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/3</td>\n",
       "      <td>[defined]</td>\n",
       "      <td>7.3</td>\n",
       "      <td>[Bacteria]</td>\n",
       "      <td>[40]</td>\n",
       "      <td>[https://mediadive.dsmz.de/strains/medium/3]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>DSMZ</td>\n",
       "      <td>ALLANTOIN MINERAL MEDIUM</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/6</td>\n",
       "      <td>[complex]</td>\n",
       "      <td></td>\n",
       "      <td>[Bacteria]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[https://mediadive.dsmz.de/strains/medium/6]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>P5</td>\n",
       "      <td>public</td>\n",
       "      <td>RS Medium - Nutrient Medium (NM) Component</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/P5</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/P5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>P6</td>\n",
       "      <td>public</td>\n",
       "      <td>mTA10</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/P6</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/P6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>P7</td>\n",
       "      <td>public</td>\n",
       "      <td>N27 RHODOSPIRILLACEAE MEDIUM (modified)</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/P7</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>6.8</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/P7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>P8</td>\n",
       "      <td>public</td>\n",
       "      <td>M9-SNG medium</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/P8</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/P8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>P9</td>\n",
       "      <td>public</td>\n",
       "      <td>PDB-SNG medium</td>\n",
       "      <td>https://mediadive.dsmz.de/medium/P9</td>\n",
       "      <td>[complex]</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://mediadive.dsmz.de/pdf/P9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3313 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:14:58.211791Z",
     "start_time": "2024-06-01T18:14:58.189925Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f5af7b6986f271f6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7bfd69ef74d4095d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
