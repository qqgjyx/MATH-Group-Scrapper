{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:14.550899Z",
     "start_time": "2024-05-30T07:04:15.597022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = 'https://mediadive.dsmz.de/gas'\n",
    "\n",
    "# Prepare lists to hold the data\n",
    "N2 = []\n",
    "O2 = []\n",
    "CO2 = []\n",
    "H2 = []\n",
    "CH4 = []\n",
    "CO = []\n",
    "Air = []\n",
    "mediums = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the table rows\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Check if rows are empty (end of pagination)\n",
    "    if not rows:\n",
    "        return False\n",
    "    \n",
    "    table_rows = soup.find('div', class_=\"table-responsive my-15\").find_all('tr')[1:]  # Skip the header row\n",
    "    # print(table_rows)\n",
    "    # Loop through the rows and extract data\n",
    "    for row in table_rows:\n",
    "        columns = row.find_all('td')\n",
    "        Medium = columns[0].get_text(strip=True)\n",
    "        mediums.append(Medium)\n",
    "        N_2 = columns[1].get_text(strip=True)\n",
    "        N2.append(N_2)\n",
    "        O_2 = columns[2].get_text(strip=True)\n",
    "        O2.append(O_2)\n",
    "        CO_2 = columns[3].get_text(strip=True)\n",
    "        CO2.append(CO_2)\n",
    "        # print(Medium)\n",
    "        H_2 = columns[4].get_text(strip=True)\n",
    "        H2.append(H_2)\n",
    "        CH_4 = columns[5].get_text(strip=True)\n",
    "        CH4.append(CH_4)\n",
    "        CO1 = columns[6].get_text(strip=True)\n",
    "        CO.append(CO1)\n",
    "        air = columns[7].get_text(strip=True)\n",
    "        Air.append(air)\n",
    "    return True\n",
    "\n",
    "# Loop through the first 100 pages\n",
    "for page in range(1, 49):\n",
    "    page_url = f'{base_url}?p={page}'\n",
    "    print(f'Scraping page: {page}')\n",
    "    if not scrape_page(page_url):\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Medium': mediums,\n",
    "    'N2':N2,\n",
    "    'O2':O2,\n",
    "    'CO2':CO2,\n",
    "    'H2':H2,\n",
    "    'CH4':CH4,\n",
    "    'CO':CO,\n",
    "    'Air':Air\n",
    "})\n",
    "\n",
    "df\n",
    "df.to_csv('data.csv')"
   ],
   "id": "73d2d473739d9e95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: 1\n",
      "Scraping page: 2\n",
      "Scraping page: 3\n",
      "Scraping page: 4\n",
      "Scraping page: 5\n",
      "Scraping page: 6\n",
      "Scraping page: 7\n",
      "Scraping page: 8\n",
      "Scraping page: 9\n",
      "Scraping page: 10\n",
      "Scraping page: 11\n",
      "Scraping page: 12\n",
      "Scraping page: 13\n",
      "Scraping page: 14\n",
      "Scraping page: 15\n",
      "Scraping page: 16\n",
      "Scraping page: 17\n",
      "Scraping page: 18\n",
      "Scraping page: 19\n",
      "Scraping page: 20\n",
      "Scraping page: 21\n",
      "Scraping page: 22\n",
      "Scraping page: 23\n",
      "Scraping page: 24\n",
      "Scraping page: 25\n",
      "Scraping page: 26\n",
      "Scraping page: 27\n",
      "Scraping page: 28\n",
      "Scraping page: 29\n",
      "Scraping page: 30\n",
      "Scraping page: 31\n",
      "Scraping page: 32\n",
      "Scraping page: 33\n",
      "Scraping page: 34\n",
      "Scraping page: 35\n",
      "Scraping page: 36\n",
      "Scraping page: 37\n",
      "Scraping page: 38\n",
      "Scraping page: 39\n",
      "Scraping page: 40\n",
      "Scraping page: 41\n",
      "Scraping page: 42\n",
      "Scraping page: 43\n",
      "Scraping page: 44\n",
      "Scraping page: 45\n",
      "Scraping page: 46\n",
      "Scraping page: 47\n",
      "Scraping page: 48\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T07:37:45.853484Z",
     "start_time": "2024-05-30T07:37:07.197722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = 'https://mediadive.dsmz.de/steps'\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page):\n",
    "    page_url = f'{base_url}?p={page}'\n",
    "    response = requests.get(page_url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the table rows\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Check if rows are empty (end of pagination)\n",
    "    if not rows:\n",
    "        return None, page\n",
    "    \n",
    "    table_rows = soup.find('div', class_=\"table-responsive my-15\").find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    # Temporary lists to hold data for this page\n",
    "    temp_ids = []\n",
    "    temp_steps = []\n",
    "    temp_solutions = []\n",
    "    temp_solutions_link = []\n",
    "    temp_mediums = []\n",
    "    temp_mediums_link = []\n",
    "\n",
    "    # Loop through the rows and extract data\n",
    "    for row in table_rows:\n",
    "        id = row.get('id')\n",
    "        temp_ids.append(id)\n",
    "        \n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        Step = columns[0].get_text(strip=True)\n",
    "        temp_steps.append(Step)\n",
    "        \n",
    "        Solution = columns[1].get_text(strip=True)\n",
    "        temp_solutions.append(Solution)\n",
    "        Solution_link = columns[1].find('a')['href'] if columns[1].find('a') else None\n",
    "        temp_solutions_link.append(Solution_link)\n",
    "        \n",
    "        Medium = columns[2].get_text(strip=True)\n",
    "        temp_mediums.append(Medium)\n",
    "        Medium_link = columns[2].find('a')['href'] if columns[2].find('a') else None\n",
    "        temp_mediums_link.append(Medium_link)\n",
    "    \n",
    "    return (temp_ids, temp_steps, temp_solutions, temp_solutions_link, temp_mediums, temp_mediums_link), page\n",
    "\n",
    "# Number of threads\n",
    "num_threads = 10\n",
    "\n",
    "# Dictionary to hold the results from each page\n",
    "results_dict = {}\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize the scraping process\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = {executor.submit(scrape_page, page): page for page in range(1, 308)}\n",
    "    with tqdm(total=307, desc=\"Scraping pages\", position=0, leave=True) as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result, page = future.result()\n",
    "            if result:\n",
    "                results_dict[page] = result\n",
    "            pbar.update(1)\n",
    "\n",
    "# Lists to hold the final results in order\n",
    "ids = []\n",
    "Steps = []\n",
    "Solutions = []\n",
    "Solutions_link = []\n",
    "Mediums = []\n",
    "Mediums_link = []\n",
    "\n",
    "# Concatenate results in the order of pages\n",
    "with tqdm(total=307, desc=\"Concatenating results\", position=1, leave=True) as pbar:\n",
    "    for page in range(1, 308):\n",
    "        if page in results_dict:\n",
    "            temp_ids, temp_steps, temp_solutions, temp_solutions_link, temp_mediums, temp_mediums_link = results_dict[page]\n",
    "            ids.extend(temp_ids)\n",
    "            Steps.extend(temp_steps)\n",
    "            Solutions.extend(temp_solutions)\n",
    "            Solutions_link.extend(temp_solutions_link)\n",
    "            Mediums.extend(temp_mediums)\n",
    "            Mediums_link.extend(temp_mediums_link)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Step': Steps,\n",
    "    'Solution': Solutions,\n",
    "    'Solution_link': Solutions_link,\n",
    "    'Medium': Mediums,\n",
    "    'Mediums_link': Mediums_link\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('Steps_new.csv', index=False)\n",
    "\n",
    "print('Scraping complete. Data saved to Steps_new.csv')"
   ],
   "id": "ac9baaaa6b3015ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 307/307 [00:38<00:00,  7.95it/s]\n",
      "\n",
      "Concatenating results: 100%|██████████| 307/307 [00:00<00:00, 224095.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to Steps_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "21c8c26fc81fe8b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
