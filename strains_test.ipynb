{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T09:21:12.132380Z",
     "start_time": "2024-06-14T09:21:11.872511Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:23:20.988418Z",
     "start_time": "2024-06-14T09:21:12.142414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Base URL of the webpage\n",
    "base_url = \"https://mediadive.dsmz.de\""
   ],
   "id": "7636cd8bec229419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "link_data_temp = list\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self, link, soup, method):\n",
    "        self.link = link\n",
    "        self.soup = soup\n",
    "        self.method = method\n",
    "        \n",
    "    def extract(self):\n",
    "        return self.method(self.link, self.soup)\n",
    "\n",
    "\n",
    "def extract_data_from_link(link, extractor_method, pbar, new_names, retries=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            extractor = Extractor(link, soup, extractor_method)\n",
    "            data = extractor.extract()\n",
    "            if data is None:\n",
    "                print('No soup for {}'.format(link))\n",
    "                # pbar.set_description('Failed to extract data from {}'.format(link))\n",
    "                data = [link].extend([None]*(len(new_names)-1))\n",
    "            pbar.update(1)\n",
    "            return data\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Error fetching link {link}: {e}. Retrying {attempt + 1}/{retries}...\")\n",
    "            time.sleep(1)  # Wait before retrying\n",
    "    return [link].extend([None]*(len(new_names)-1))\n",
    "\n",
    "# Function to extract data from a single page\n",
    "def extract_data_from_page(link, page, extractor_method, pbar, new_names, retries=5):\n",
    "    params = {\"p\": page}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(link, params=params)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            extractor = Extractor(link, soup, extractor_method)\n",
    "            data = extractor.extract()\n",
    "            if data is None:\n",
    "                print('No soup for {}'.format(link + str(page)))\n",
    "                # pbar.set_description('Failed to extract data from {}'.format(link))\n",
    "                data = [page].extend([None]*(len(new_names)-1))\n",
    "            pbar.update(1)\n",
    "            return data\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Error fetching page {page}: {e}. Retrying {attempt + 1}/{retries}...\")\n",
    "            time.sleep(1)  # Wait before retrying\n",
    "    return [page].extend([None]*(len(new_names)-1))\n",
    "\n",
    "class MyScrapper:\n",
    "    def __init__(self, old_df, on_old_name, extractor_method, new_names, max_workers=128):\n",
    "        self.on_old_name = on_old_name\n",
    "        self.base_url = None\n",
    "        \n",
    "        self.extractor_method = extractor_method\n",
    "        self.new_names = new_names\n",
    "        self.data_temp = []\n",
    "        \n",
    "        self.merged_df = pd.DataFrame()\n",
    "        self.old_df = old_df\n",
    "        \n",
    "        self.links = old_df[on_old_name].dropna().unique()\n",
    "        self.pages = None\n",
    "        \n",
    "        self.max_workers = max_workers\n",
    "        print(\"Scrapper ready on \" + on_old_name + \"for {n} links\".format(n=len(self.links)))\n",
    "    \n",
    "    def __init__(self, base_url, pages, extractor_method, new_names, max_workers=128):\n",
    "        self.on_old_name = None\n",
    "        self.base_url = base_url\n",
    "        \n",
    "        self.extractor_method = extractor_method\n",
    "        self.new_names = new_names\n",
    "        self.data_temp = []\n",
    "        \n",
    "        self.merged_df = pd.DataFrame()\n",
    "        self.old_df = pd.DataFrame()\n",
    "        \n",
    "        self.links = None\n",
    "        self.pages = pages\n",
    "        self.max_workers = max_workers\n",
    "        print(\"Scrapper ready on \" + new_names + \"for {n} pages\".format(n=pages))\n",
    "        \n",
    "    def set_max_workers(self, max_workers):\n",
    "        self.max_workers = max_workers\n",
    "        \n",
    "    def to_merge(self, on_old_name, extractor_method, new_names, pages=None):\n",
    "        try:\n",
    "            if self.old_df.empty:\n",
    "                raise ValueError(\"Old dataframe is empty\")\n",
    "            \n",
    "            self.on_old_name = on_old_name\n",
    "            self.base_url = None\n",
    "            \n",
    "            self.extractor_method = extractor_method\n",
    "            self.new_names = new_names\n",
    "            self.data_temp = []\n",
    "            \n",
    "            self.merged_df = pd.DataFrame()\n",
    "            self.old_df = self.merged_df if self.merged_df is not None else self.old_df\n",
    "            \n",
    "            self.links = self.old_df[on_old_name].dropna().unique()\n",
    "            self.pages = pages\n",
    "            print(\"Scrapper switched ready on \" + on_old_name + \"for {n} links\".format(n=len(self.links)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return\n",
    "        \n",
    "    # Function to extract data from a single page\n",
    "\n",
    "    def scrape(self):\n",
    "        try:\n",
    "            on_old_name = self.on_old_name\n",
    "            base_url = self.base_url\n",
    "            \n",
    "            extractor_method = self.extractor_method\n",
    "            new_names = self.new_names\n",
    "            data_temp = self.data_temp\n",
    "            \n",
    "            merged_df = self.merged_df\n",
    "            old_df = self.old_df\n",
    "            \n",
    "            links = self.links\n",
    "            pages = self.pages\n",
    "            \n",
    "            if not merged_df.empty:\n",
    "                raise ValueError(\"Merged dataframe is not cleared out\")\n",
    "            \n",
    "            if links:\n",
    "                print(\"Scrapping links\")\n",
    "                with tqdm(total=len(links), desc=on_old_name+\" link Progress\") as pbar:\n",
    "                    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                        future_to_link = {executor.submit(extract_data_from_link, link, extractor_method, pbar, new_names): link for link in links}\n",
    "                        for future in as_completed(future_to_link):\n",
    "                            data = future.result()\n",
    "                            self.data_temp.append(data)\n",
    "                    link_df = pd.DataFrame(data_temp, columns=new_names)\n",
    "                    self.merged_df = pd.merge(old_df, link_df, on=on_old_name, how='left')\n",
    "                return self.merged_df\n",
    "            \n",
    "            elif pages:\n",
    "                print(\"Scrapping pages\")\n",
    "                with tqdm(total=pages, desc=base_url+\" page Progress\") as pbar:\n",
    "                    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                        future_to_page = {executor.submit(extract_data_from_page, base_url, page, extractor_method, pbar, new_names): page for page in range(1, pages + 1)}\n",
    "                        for future in as_completed(future_to_page):\n",
    "                            data = future.result()\n",
    "                            data_temp.extend(data)\n",
    "                    page_df = pd.DataFrame(data_temp.sort(key=lambda x: x[0]), columns=new_names).drop(columns=[\"Page\"], inplace=True)\n",
    "                    self.old_df = page_df\n",
    "                return self.old_df\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ],
   "id": "66a7200d65c10a62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data from pages: 100%|██████████| 2314/2314 [02:08<00:00, 17.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      Organism Group                                               Name  \\\n",
       "0          Bacterium                       Heyndrickxia coagulans DSM 1   \n",
       "1          Bacterium  Paenibacillus macquariensis subsp. macquariens...   \n",
       "2          Bacterium                    Sporosarcina psychrophila DSM 3   \n",
       "3          Bacterium                      Sporosarcina globispora DSM 4   \n",
       "4          Bacterium                    Psychrobacillus insolitus DSM 5   \n",
       "...              ...                                                ...   \n",
       "46258                                         Phage (phagum) DSM 117437   \n",
       "46259                                         Phage (phagum) DSM 117679   \n",
       "46260                                         Phage (phagum) DSM 117680   \n",
       "46261                             Staphylococcus epidermidis DSM 117681   \n",
       "46262                             Staphylococcus epidermidis DSM 117682   \n",
       "\n",
       "                                               Name Link DSM No.  \\\n",
       "0           https://mediadive.dsmz.de/strains/view/DSM 1       1   \n",
       "1           https://mediadive.dsmz.de/strains/view/DSM 2       2   \n",
       "2           https://mediadive.dsmz.de/strains/view/DSM 3       3   \n",
       "3           https://mediadive.dsmz.de/strains/view/DSM 4       4   \n",
       "4           https://mediadive.dsmz.de/strains/view/DSM 5       5   \n",
       "...                                                  ...     ...   \n",
       "46258  https://mediadive.dsmz.de/strains/view/DSM 117437  117437   \n",
       "46259  https://mediadive.dsmz.de/strains/view/DSM 117679  117679   \n",
       "46260  https://mediadive.dsmz.de/strains/view/DSM 117680  117680   \n",
       "46261  https://mediadive.dsmz.de/strains/view/DSM 117681  117681   \n",
       "46262  https://mediadive.dsmz.de/strains/view/DSM 117682  117682   \n",
       "\n",
       "                                           Taxonomy Link   Growth media  \\\n",
       "0      https://mediadive.dsmz.de/taxonomy?level=speci...  [453, 1, J22]   \n",
       "1      https://mediadive.dsmz.de/taxonomy?level=speci...            [1]   \n",
       "2      https://mediadive.dsmz.de/taxonomy?level=speci...       [1, J22]   \n",
       "3      https://mediadive.dsmz.de/taxonomy?level=speci...     [514, J22]   \n",
       "4      https://mediadive.dsmz.de/taxonomy?level=speci...          [123]   \n",
       "...                                                  ...            ...   \n",
       "46258                                               None          [381]   \n",
       "46259                                               None           [92]   \n",
       "46260                                               None           [92]   \n",
       "46261                                               None           [92]   \n",
       "46262                                               None           [92]   \n",
       "\n",
       "                                      Growth Media Links  \\\n",
       "0      [https://mediadive.dsmz.de/medium/453, https:/...   \n",
       "1                   [https://mediadive.dsmz.de/medium/1]   \n",
       "2      [https://mediadive.dsmz.de/medium/1, https://m...   \n",
       "3      [https://mediadive.dsmz.de/medium/514, https:/...   \n",
       "4                 [https://mediadive.dsmz.de/medium/123]   \n",
       "...                                                  ...   \n",
       "46258             [https://mediadive.dsmz.de/medium/381]   \n",
       "46259              [https://mediadive.dsmz.de/medium/92]   \n",
       "46260              [https://mediadive.dsmz.de/medium/92]   \n",
       "46261              [https://mediadive.dsmz.de/medium/92]   \n",
       "46262              [https://mediadive.dsmz.de/medium/92]   \n",
       "\n",
       "                                          external links  \\\n",
       "0      [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "1      [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "2      [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "3      [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "4      [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "...                                                  ...   \n",
       "46258  [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "46259  [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "46260  [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "46261  [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "46262  [https://www.dsmz.de/collection/catalogue/deta...   \n",
       "\n",
       "                                          DSMZ Catalogue  \\\n",
       "0      https://www.dsmz.de/collection/catalogue/detai...   \n",
       "1      https://www.dsmz.de/collection/catalogue/detai...   \n",
       "2      https://www.dsmz.de/collection/catalogue/detai...   \n",
       "3      https://www.dsmz.de/collection/catalogue/detai...   \n",
       "4      https://www.dsmz.de/collection/catalogue/detai...   \n",
       "...                                                  ...   \n",
       "46258  https://www.dsmz.de/collection/catalogue/detai...   \n",
       "46259  https://www.dsmz.de/collection/catalogue/detai...   \n",
       "46260  https://www.dsmz.de/collection/catalogue/detai...   \n",
       "46261  https://www.dsmz.de/collection/catalogue/detai...   \n",
       "46262  https://www.dsmz.de/collection/catalogue/detai...   \n",
       "\n",
       "                               Bacdive Link  \n",
       "0        https://bacdive.dsmz.de/strain/654  \n",
       "1      https://bacdive.dsmz.de/strain/11477  \n",
       "2      https://bacdive.dsmz.de/strain/11984  \n",
       "3      https://bacdive.dsmz.de/strain/11976  \n",
       "4       https://bacdive.dsmz.de/strain/1565  \n",
       "...                                     ...  \n",
       "46258                                  None  \n",
       "46259                                  None  \n",
       "46260                                  None  \n",
       "46261                                  None  \n",
       "46262                                  None  \n",
       "\n",
       "[46263 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organism Group</th>\n",
       "      <th>Name</th>\n",
       "      <th>Name Link</th>\n",
       "      <th>DSM No.</th>\n",
       "      <th>Taxonomy Link</th>\n",
       "      <th>Growth media</th>\n",
       "      <th>Growth Media Links</th>\n",
       "      <th>external links</th>\n",
       "      <th>DSMZ Catalogue</th>\n",
       "      <th>Bacdive Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacterium</td>\n",
       "      <td>Heyndrickxia coagulans DSM 1</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://mediadive.dsmz.de/taxonomy?level=speci...</td>\n",
       "      <td>[453, 1, J22]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/453, https:/...</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>https://bacdive.dsmz.de/strain/654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacterium</td>\n",
       "      <td>Paenibacillus macquariensis subsp. macquariens...</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://mediadive.dsmz.de/taxonomy?level=speci...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/1]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>https://bacdive.dsmz.de/strain/11477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bacterium</td>\n",
       "      <td>Sporosarcina psychrophila DSM 3</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 3</td>\n",
       "      <td>3</td>\n",
       "      <td>https://mediadive.dsmz.de/taxonomy?level=speci...</td>\n",
       "      <td>[1, J22]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/1, https://m...</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>https://bacdive.dsmz.de/strain/11984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bacterium</td>\n",
       "      <td>Sporosarcina globispora DSM 4</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 4</td>\n",
       "      <td>4</td>\n",
       "      <td>https://mediadive.dsmz.de/taxonomy?level=speci...</td>\n",
       "      <td>[514, J22]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/514, https:/...</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>https://bacdive.dsmz.de/strain/11976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bacterium</td>\n",
       "      <td>Psychrobacillus insolitus DSM 5</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 5</td>\n",
       "      <td>5</td>\n",
       "      <td>https://mediadive.dsmz.de/taxonomy?level=speci...</td>\n",
       "      <td>[123]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/123]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>https://bacdive.dsmz.de/strain/1565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46258</th>\n",
       "      <td></td>\n",
       "      <td>Phage (phagum) DSM 117437</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 117437</td>\n",
       "      <td>117437</td>\n",
       "      <td>None</td>\n",
       "      <td>[381]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/381]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46259</th>\n",
       "      <td></td>\n",
       "      <td>Phage (phagum) DSM 117679</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 117679</td>\n",
       "      <td>117679</td>\n",
       "      <td>None</td>\n",
       "      <td>[92]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/92]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46260</th>\n",
       "      <td></td>\n",
       "      <td>Phage (phagum) DSM 117680</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 117680</td>\n",
       "      <td>117680</td>\n",
       "      <td>None</td>\n",
       "      <td>[92]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/92]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46261</th>\n",
       "      <td></td>\n",
       "      <td>Staphylococcus epidermidis DSM 117681</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 117681</td>\n",
       "      <td>117681</td>\n",
       "      <td>None</td>\n",
       "      <td>[92]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/92]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46262</th>\n",
       "      <td></td>\n",
       "      <td>Staphylococcus epidermidis DSM 117682</td>\n",
       "      <td>https://mediadive.dsmz.de/strains/view/DSM 117682</td>\n",
       "      <td>117682</td>\n",
       "      <td>None</td>\n",
       "      <td>[92]</td>\n",
       "      <td>[https://mediadive.dsmz.de/medium/92]</td>\n",
       "      <td>[https://www.dsmz.de/collection/catalogue/deta...</td>\n",
       "      <td>https://www.dsmz.de/collection/catalogue/detai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46263 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "source": [
    "\n",
    "\n",
    "# Function to extract data from a single page\n",
    "def extract_data_from_page(url, page, retries=5):\n",
    "    params = {\"p\": page}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            data = []\n",
    "            table_rows = soup.find_all('tr')[1:]  # Skip the header row\n",
    "            for row in table_rows:\n",
    "                columns = row.find_all('td')\n",
    "                row_data = [page]  # Add the page number\n",
    "                \n",
    "                # Organism Group\n",
    "                organism_group = columns[0].get_text(strip=True)\n",
    "                row_data.append(organism_group)\n",
    "                \n",
    "                # Name and link\n",
    "                name_tag = columns[1].find('a')\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                name_link = base_url + name_tag['href']\n",
    "                row_data.append(name)\n",
    "                row_data.append(name_link)\n",
    "                \n",
    "                dsm_no = name.split(' ')[-1]\n",
    "                row_data.append(dsm_no)\n",
    "                \n",
    "                # Taxonomy link\n",
    "                taxonomy_tag = columns[2].find('a')\n",
    "                taxonomy_link = base_url + taxonomy_tag['href'] if taxonomy_tag else None\n",
    "                row_data.append(taxonomy_link)\n",
    "                \n",
    "                # Growth Media links\n",
    "                growth_media = [a.get_text(strip=True) for a in columns[3].find_all('a')]\n",
    "                growth_media_links = [base_url + a['href'] for a in columns[3].find_all('a')]\n",
    "                row_data.append(growth_media)\n",
    "                row_data.append(growth_media_links)\n",
    "                \n",
    "                # External links\n",
    "                external_links = [a['href'] for a in columns[4].find_all('a')] if columns[4].find('a') else None\n",
    "                row_data.append(external_links)\n",
    "                dsmz_catalogue = columns[4].find_all('a')[0]['href'] if \"dsmz\" in columns[4].find_all('a')[0]['href'] else None\n",
    "                bacdive_link = columns[4].find_all('a')[1]['href'] if len(columns[4].find_all('a')) > 1 and \"bacdive\" in columns[4].find_all('a')[1]['href'] else None\n",
    "                row_data.append(dsmz_catalogue)\n",
    "                row_data.append(bacdive_link)\n",
    "                \n",
    "                data.append(row_data)\n",
    "\n",
    "            return data\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Error fetching page {page}: {e}. Retrying {attempt + 1}/{retries}...\")\n",
    "            time.sleep(1)  # Wait before retrying\n",
    "    return []\n",
    "\n",
    "\n",
    "# Main scraping process\n",
    "all_data = []\n",
    "num_pages = 2#314  # Adjust the number of pages you want to scrape\n",
    "\n",
    "max_workers = 128  # Adjust based on the MacBook M3 Pro capabilities\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_page = {executor.submit(extract_data_from_page, base_url + \"/strains\", page): page for page in\n",
    "                      range(1, num_pages + 1)}\n",
    "\n",
    "    for future in tqdm(as_completed(future_to_page), total=num_pages, desc=\"Extracting data from pages\"):\n",
    "        page_data = future.result()\n",
    "        all_data.extend(page_data)\n",
    "\n",
    "# Sort the data based on the page number to maintain the order\n",
    "all_data.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "columns = [\"Page\", \"Organism Group\", \"Name\", \"Name Link\", \"DSM No.\",\"Taxonomy Link\", \"Growth media\", \"Growth Media Links\", \"external links\", \"DSMZ Catalogue\", \"Bacdive Link\"]\n",
    "df = pd.DataFrame(all_data, columns=columns)\n",
    "df.drop(columns=[\"Page\"], inplace=True)  # Remove the page column if not needed\n",
    "\n",
    "df"
   ],
   "id": "48871918fa4020fb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-14T09:23:20.989188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to fetch HTML content and parse it with BeautifulSoup\n",
    "def fetch_html_structure(url, retries=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Error fetching HTML content from {url}: {e}. Retrying {attempt + 1}/{retries}...\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to extract key data from the parsed HTML content\n",
    "def extract_key_data(soup):\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    title = soup.find('title').text.strip() if soup.find('title') else 'N/A'\n",
    "    strain_name = soup.find('h2').text.strip() if soup.find('h2') else 'N/A'\n",
    "    synonyms = []\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for p in paragraphs:\n",
    "        bold_text = p.find('b', string='Synonyms:')\n",
    "        if bold_text:\n",
    "            for content in p.contents:\n",
    "                if content.name == 'a':\n",
    "                    synonyms.append(content.get_text(strip=True))\n",
    "                    synonyms.append('href: ' + content.get('href'))\n",
    "                elif isinstance(content, str) and content.strip():\n",
    "                    synonyms.extend(content.strip().split(', '))\n",
    "            break\n",
    "    \n",
    "    media_details = []\n",
    "    media_boxes = soup.find_all('div', class_='box')\n",
    "    for box in media_boxes:\n",
    "        media_title = box.find('h3', class_='title').text.strip() if box.find('h3', 'title') else 'N/A'\n",
    "        media_link = box.find('a', class_='link colorless')['href'] if box.find('a', 'link colorless') else 'N/A'\n",
    "        growth_observed = 'Yes' if box.find('i', class_='ph ph-lg ph-check text-success') else 'No'\n",
    "        growth_conditions = box.find('span', class_='badge danger').text.strip() if box.find('span', 'badge danger') else 'N/A'\n",
    "        \n",
    "        media_details.append({\n",
    "            'media_title': media_title,\n",
    "            'media_link': media_link,\n",
    "            'growth_observed': growth_observed,\n",
    "            'growth_conditions': growth_conditions\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'strain_name': strain_name,\n",
    "        'synonyms': synonyms,\n",
    "        'media_details': media_details\n",
    "    }\n",
    "\n",
    "# Function to extract strain details from a row of the DataFrame\n",
    "def extract_strain_details(row):\n",
    "    url = row['Name Link']\n",
    "    soup = fetch_html_structure(url)\n",
    "    return extract_key_data(soup)\n",
    "\n",
    "# Load the initial DataFrame (assuming it's loaded in variable df)\n",
    "# df = pd.read_csv('initial_data.csv')  # Load your initial DataFrame here\n",
    "\n",
    "# Initialize an empty list to hold detailed data\n",
    "detailed_data = []\n",
    "num_strains = df.shape[0]\n",
    "max_workers = 10  # Adjust the number of workers as needed\n",
    "\n",
    "# Extract detailed information for each strain with ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_index = {executor.submit(extract_strain_details, row): index for index, row in df.iterrows()}\n",
    "    \n",
    "    for future in tqdm(as_completed(future_to_index), total=num_strains, desc=\"Extracting strain details\"):\n",
    "        index = future_to_index[future]\n",
    "        strain_details = future.result()\n",
    "        if strain_details:\n",
    "            detailed_data.append({\n",
    "                \"Name\": df.loc[index, 'Name'],\n",
    "                \"Synonyms\": ', '.join(strain_details['synonyms']),\n",
    "                \"Growth Conditions\": strain_details['media_details']\n",
    "            })\n",
    "            \n",
    "\n",
    "# # Sort the detailed data based on the Name to maintain order\n",
    "# detailed_data.sort(key=lambda x: int(x['Name'].split(' ')[-1]))\n",
    "\n",
    "# Create the final detailed DataFrame\n",
    "df_detailed = pd.DataFrame(detailed_data)\n",
    "merged_df = pd.merge(df, df_detailed, on=\"Name\", how='left')\n",
    "\n",
    "merged_df"
   ],
   "id": "b1f2394e09dfeda9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def dsmzlink_method(dsmzlink, soup):\n",
    "    soup = soup.find('div', class_='product-detail') if soup.find('div', class_='product-detail') else None\n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    def get_field_value(soup, label):\n",
    "        field = soup.find('div', class_='label', string=lambda x: x and label in x)\n",
    "        if field:\n",
    "            return '\"' + field.find_next_sibling('div', class_='value').get_text(strip=True) + '\"'\n",
    "        return None\n",
    "        \n",
    "    name_value = get_field_value(soup, 'Name: ')\n",
    "    sd_value = get_field_value(soup, 'Strain designation: ')\n",
    "    \n",
    "    dsm_field = soup.find('div', class_='label', string=lambda x: x and 'DSM No.: ' in x) if soup.find('div', class_='label', string=lambda x: x and 'DSM No.: ' in x) else None\n",
    "    if dsm_field:\n",
    "        dsm_value = dsm_field.find_next_sibling('div', class_='value').get_text(strip=True) \n",
    "        type_strain = \"yes\" if \"Type strain\" in dsm_value else \"no\"\n",
    "    else:\n",
    "        type_strain = \"No\"\n",
    "        \n",
    "    def get_comp_value(soup, label):\n",
    "        field = soup.select_one('div.label:-soup-contains(\"'+label+'\")')\n",
    "        if field:\n",
    "            return field.find_next_sibling('div', class_='value')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    other_value = get_comp_value(soup,\"Other collection no.\" )\n",
    "    if other_value:\n",
    "        other_value = '\"'+other_value.get_text(strip=True)+'\"'\n",
    "        \n",
    "    iso_value = get_field_value(soup, 'Isolated from: ')\n",
    "    country_value = get_field_value(soup, 'Country: ')\n",
    "    date_value = get_field_value(soup, 'Date of sampling: ')\n",
    "    \n",
    "    risk_tag = get_comp_value(soup, 'Risk group: ')\n",
    "    if risk_tag:\n",
    "        risk_value = '\"' + risk_tag.get_text(strip=True) + '\"'\n",
    "    else:\n",
    "        risk_value = None\n",
    "    risk_group = risk_tag.get_text(strip=True).split(' ')[0].split('(')[0] if risk_tag else None\n",
    "    class_by = risk_tag.find('a').get_text(strip=True) if risk_tag.find('a') else risk_tag.get_text(strip=True).split(' ')[-1].replace(')','')\n",
    "    \n",
    "    nagoya_value = get_field_value(soup, 'Nagoya Protocol Restrictions: ')\n",
    "    history_value = get_field_value(soup, 'History: ')\n",
    "    \n",
    "    genbank_tag = get_comp_value(soup, 'Genbank accession numbers: ')\n",
    "    if genbank_tag:\n",
    "        genbank_value = '\"' + genbank_tag.get_text(strip=True) + '\"'\n",
    "    else:\n",
    "        genbank_value = None\n",
    "    \n",
    "    # Initialize the dictionary\n",
    "    sequence_dict = {}\n",
    "    # Extract information\n",
    "    if genbank_tag:\n",
    "        text_taw = genbank_tag.get_text(separator=\"|\").split(\"|\")\n",
    "        text = [s for s in text_taw if \":\" in s]\n",
    "        links = genbank_tag.find_all('a')\n",
    "        for i, part in enumerate(text):\n",
    "            if ':' in part:\n",
    "                key = part.strip()\n",
    "                value = [links[i].text, links[i]['href']]\n",
    "                sequence_dict[key] = value\n",
    "    whole_genome_tag = sequence_dict.get(\"whole genome shotgun sequence:\")\n",
    "    if whole_genome_tag:\n",
    "        whole_genome = whole_genome_tag[0]\n",
    "        whole_genome_link = whole_genome_tag[1]\n",
    "    else:\n",
    "        whole_genome = None\n",
    "        whole_genome_link = None\n",
    "    sixteens_rrna_tag = sequence_dict.get(\"16S rRNA gene:\")\n",
    "    if sixteens_rrna_tag:\n",
    "        sixteens_rrna = sixteens_rrna_tag[0]\n",
    "        sixteens_rrna_link = sixteens_rrna_tag[1]\n",
    "        if sixteens_rrna_link != '' and sixteens_rrna_link is not None:\n",
    "            sixteens_rrna_link = sixteens_rrna_link + '.1?report=fasta'\n",
    "    else:\n",
    "        sixteens_rrna = None\n",
    "        sixteens_rrna_link = None\n",
    "        \n",
    "    additional_tag = get_comp_value(soup, 'additional information: ')\n",
    "    if additional_tag:\n",
    "        additional_value = '\"' + additional_tag.get_text(strip=True) + '\"'\n",
    "    else:\n",
    "        additional_value = None\n",
    "        \n",
    "    literature_value = get_field_value(soup, \"Literature: \")\n",
    "    \n",
    "    wink_tag = get_comp_value(soup, \"Wink compendium: \")\n",
    "    if wink_tag:\n",
    "        wink_value = '\"' + wink_tag.get_text(strip=True) + '\"'\n",
    "        wink_link = wink_tag.find('a')['href']\n",
    "    else:\n",
    "        wink_value = None\n",
    "        wink_link = None\n",
    "    \n",
    "    supplied_tag = get_comp_value(soup, \"Supplied as: \")\n",
    "    supplied_dic = {}\n",
    "    price_category = None\n",
    "    if supplied_tag:\n",
    "        supplied_value = '\"' + supplied_tag.get_text(strip=True) + '\"'\n",
    "        table = supplied_tag.find('table')\n",
    "        trs = table.find_all('tr')[2:]\n",
    "        for tr in trs:\n",
    "            td = tr.find_all('td')\n",
    "            if len(td) == 4:\n",
    "                delivery_form = td[0].get_text(strip=True)\n",
    "                prices = td[-2].get_text(strip=True)\n",
    "                supplied_dic[delivery_form] = prices\n",
    "            if len(td) == 3:\n",
    "                delivery_form = td[0].get_text(strip=True)\n",
    "                prices = td[-1].get_text(strip=True)\n",
    "                supplied_dic[delivery_form] = prices\n",
    "            if len(td) == 1:\n",
    "                price_category = td[0].find('b').get_text(strip=True) if td[0].find('b') else None\n",
    "    else:\n",
    "        supplied_value = None\n",
    "        price_category = None\n",
    "    freeze_dried = supplied_dic.get(\"Freeze Dried\")\n",
    "    active_culture = supplied_dic.get(\"Active culture on request\")\n",
    "    dna_price = supplied_dic.get(\"DNA\")\n",
    "    \n",
    "    culture_tag = get_comp_value(soup, \"Other cultures:\")\n",
    "    if culture_tag:\n",
    "        culture_link = culture_tag.find('a')['href']\n",
    "    else:\n",
    "        culture_link = None\n",
    "    \n",
    "    return [dsmzlink, name_value, sd_value, type_strain, other_value, iso_value, country_value, date_value, risk_value, risk_group, class_by, nagoya_value, history_value, genbank_value, sequence_dict, whole_genome, whole_genome_link, sixteens_rrna, sixteens_rrna_link, additional_value, literature_value, wink_value, wink_link, supplied_value, supplied_dic, freeze_dried, active_culture, dna_price, price_category, culture_link]\n",
    "\n",
    "merged_merged_df = scrape_link(merged_df, 'DSMZ Catalogue', dsmzlink_method, [\"DSMZ Catalogue\", \"Full Stain Name\", \"Strain Designation\", \"Type Strain\", \"Other collection no./WDCM no.\", \"Isolated from\", \"Country\", \"Date of sampling\", \"Risk group raw\", \"Risk group\", \"classification by\", \"Nagoya Protocol Restrictions\", \"History\", \"Genbank accession raw\", \"Genbank dict\", \"whole genome shotgun sequence no.\", \"whole genome shotgun sequence no. link\", \"16S rRNA gene no.\", \"16S rRNA gene no. link\", \"Summary and additional information\", \"Literature\", \"Wink compendium\", \"Wink compendium link\", \"Supplied as raw\", \"Supplied as dict\", \"Price of Freeze Dried\", \"Price of Active culture on request\", \"Price of DNA\", \"Price Category\", \"Culture link\"])\n",
    "\n",
    "merged_merged_df"
   ],
   "id": "22e4d4c5f724ca36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def bacdiv_method(bacdivlink, soup, retries=5):\n",
    "    if soup is None:\n",
    "        return None\n",
    "    link_div = soup.find('td', class_=\"bold_valigntop width180_valigntop\", string=\"Synonym\") if soup.find('td', class_=\"bold_valigntop width180_valigntop\", string=\"Synonym\") else None\n",
    "    if link_div:  \n",
    "        href = link_div.find_next_sibling('td').find('a')['href']\n",
    "    else:\n",
    "        return [bacdivlink, None]\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()\n",
    "            soup_2 = BeautifulSoup(response.content, 'html.parser')\n",
    "            synonyms_full = soup_2.find('p').get_text(strip=True).split(':')[-1] if \"Name\" in soup_2.find('p').get_text(strip=True) else None\n",
    "            return [bacdivlink, synonyms_full]\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Error fetching link {href}: {e}. Retrying {attempt + 1}/{retries}...\")\n",
    "            time.sleep(1)  # Wait before retrying\n",
    "    return [bacdivlink, href]\n",
    "\n",
    "merged_merged_merged_df = scrape_link(merged_merged_df, 'Bacdive Link', bacdiv_method, [\"Bacdive Link\", \"Synonyms Full\"])\n",
    "\n",
    "merged_merged_merged_df"
   ],
   "id": "87ccf0e169e5d9b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "merged_merged_merged_df.to_csv(\"strains.csv\", index=False)",
   "id": "d0623d452f035a82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e2a28a91bae035ae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
